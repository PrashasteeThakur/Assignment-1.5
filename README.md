1.Main sources of Data flood • Archives • Documents • Media • Business Applications • Social Media • Public Web • Data Storage • Machine log data • Sensor data Some of the real life examples of data flooding are:- • The New York stock exchange generates about 4-5 terabytes of data everyday • Facebook hosts more than 240 billion photos, growing at 7 petabytes of data everyday • Ancestory.com, the genealogy site stores around 10 petabytes of the data • The Internet Archive stores around 18.5 petabytes of data • The Large Hadron Collider near Geneva produces about 30 Petabytes of data every year3. Hadoop has become the solution for data explosion due to the following reasons-

2.Difference between Data and Big data. Traditional data use centralized database architecture in which large and complex problems are solved by a single computer system. Big data is based on the distributed database architecture where a large block of data is solved by dividing it into several smaller sizes.
Traditional database systems are based on the structured data i.e. traditional data is stored in fixed format or fields in a file. Examples of the unstructured data include Relational Database System (RDBMS) and the spreadsheets. Big data uses the semi-structured and unstructured data and improves the variety of the data gathered from different sources like customers, audience or subscribers
The traditional system database can store only small amount of data ranging from gigabytes to terabytes. Big data helps to store and process large amount of data which consists of hundreds of terabytes of data or petabytes of data and beyond
Big data uses the dynamic schema for data storage. The traditional database is based on the fixed schema which is static in nature.
Big data is based on the scale out architecture under which the distributed approaches for computing are employed with more than one server. So, the load of the computation is shared with single application based system. Achieving the scalability in the traditional database is very difficult because the traditional database runs on the single server and requires expensive servers to scale up

3.Hadoop is an open source, Java-based programming framework that supports the processing of large data sets in a distributed computing environment • Hadoop provides: A reliable, scalable platform for storage and analysis • It is based on Google File System or GFS • Hadoop runs a number of applications on distributed systems with thousands of nodes involving petabytes of data • It has a distributed file system, called the Hadoop Distributed File System or HDFS, which enables fast data transfer among the nodes • It leverages a distributed computation framework called MapReduce Problems with distributed processing: 1. Hardware failure: can be solved by redundancy 2. Coordinating the tasks and combining results from all machines • Hadoop takes care of the above complexities and the challenges of network/distributed programming • HDFS (for storage) • Map Reduce (for processing)
